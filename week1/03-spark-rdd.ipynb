{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb2bcbf3-dfb9-47e1-b55c-e63644ca162d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# RDD\n",
    "\n",
    "an RDD is the fundamental data structure of Apache Spark. It's a fault-tolerant, distributed collection of elements that can be operated on in parallel.\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "- Immutable\n",
    "- Lazy evaluation\n",
    "- Fault tolerant (via lineage info)\n",
    "- Partitioned across cluster nodes\n",
    "- Can be cached in memory\n",
    "\n",
    "### SparkContext and SparkConf\n",
    "\n",
    "\n",
    "SparkContext is the entry point for Spark functionality.\n",
    "\n",
    "#### `SparkConf`\n",
    "\n",
    "- Configuration for Spark application\n",
    "\n",
    "**Common settings:**\n",
    "\n",
    "- setMaster(\"local[*]\") – Use local mode with all cores\n",
    "- setAppName(\"RDDExample\") – Application name\n",
    "\n",
    "e.g.,\n",
    "\n",
    "```py\n",
    "# SparkContext and SparkConf\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"salesDemo\").setMaster(\"local[*]\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "```\n",
    "\n",
    "### transformations\n",
    "\n",
    "Transformations create a new RDD from an existing one. They are lazy – not executed until an action is triggered.\n",
    "\n",
    "| Transformation  | Description                                          |\n",
    "| --------------- | ---------------------------------------------------- |\n",
    "| `map(func)`     | Returns a new RDD by applying `func` to each element |\n",
    "| `filter(func)`  | Filters elements for which `func` returns true       |\n",
    "| `flatMap(func)` | Like map but flattens the result                     |\n",
    "| `distinct()`    | Removes duplicates                                   |\n",
    "| `union(rdd)`    | Combines two RDDs                                    |\n",
    "| `groupByKey()`  | Groups values with same key                          |\n",
    "| `reduceByKey()` | Aggregates values with same key using a function     |\n",
    "| `sortBy(func)`  | Sorts RDD by computed key                            |\n",
    "\n",
    "\n",
    "### actions\n",
    "\n",
    "Actions trigger computation and return results or write data.\n",
    "\n",
    "| Action             | Description                            |\n",
    "| ------------------ | -------------------------------------- |\n",
    "| `collect()`        | Returns all elements to driver         |\n",
    "| `count()`          | Returns number of elements             |\n",
    "| `first()`          | Returns first element                  |\n",
    "| `take(n)`          | Returns first `n` elements             |\n",
    "| `reduce(func)`     | Reduces elements using binary operator |\n",
    "| `saveAsTextFile()` | Writes RDD to text files               |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reference - [spark rdd docs](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4be44b60-d539-48c8-b775-f83ece7949c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "224bf95a-5ee9-4210-9c1b-0104d7a3cb45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "353b4018-e10e-4d11-b3b6-25185519ed72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e9d9b73-12ae-41b9-bf9b-b9e68669c128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Shared Variables\n",
    "\n",
    "When you pass a function (like in map or reduce) to Spark, that function runs on different machines in the cluster — not on your local driver.\n",
    "\n",
    "**By default:**\n",
    "\n",
    "- Spark makes separate copies of any variable you use inside those functions.\n",
    "- So, if a task changes a variable on the executor, that change will not reflect back in your driver program.\n",
    "\n",
    "This is done to keep things fast and distributed — but it also means you can not just update normal variables across tasks.\n",
    "\n",
    "**challenges:**\n",
    "\n",
    "You want to count how many rows have Amount > 500 using this code:\n",
    "\n",
    "```py\n",
    "count = 0\n",
    "\n",
    "def increment_count(x):\n",
    "    count += 1 \n",
    "\n",
    "records.filter(lambda x: int(x[2]) > 500).foreach(increment_count)\n",
    "print(count)\n",
    "\n",
    "```\n",
    "\n",
    "This will not work because each machine updates its own copy of count not the original one in the driver.\n",
    "\n",
    "To solve such issues, spark provides two types of shared variables:\n",
    "\n",
    "1. **Broadcast Variables:** a read-only variable that can be cached on each machine (executor). Used to efficiently share large data (like lookup tables) with all tasks without copying it multiple times.\n",
    "2. **Accumulators:** variables used to safely implement counters or sums across mulitple worker nodes. You can only add to them (not read or subtract inside tasks). The final value is only accessible on the driver after an action is executed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80866d1a-9583-4427-af01-99b56fa5f27f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# broadcast variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5c1e05d-1b83-45c9-897e-68a0242002f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74a76255-acd0-4d9a-8049-206cbd36a2ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Activity:**\n",
    "\n",
    "1. find all product IDs where the amount is greater than 900.\n",
    "2. Find all transactions that belong to the “Furniture” category.\n",
    "3. Count how many transactions belong to the “Electronics” category.\n",
    "4. Find average amount for each category.\n",
    "5. Find the highest amount and the corresponding product ID.\n",
    "6. Find the total number of unique categories.\n",
    "7. For each category, find the product ID with the highest sale.\n",
    "8. Count how many products were sold for less than 300.\n",
    "9. Sort the transactions by amount in descending order.\n",
    "10. Broadcast Variable: Category Discounts\n",
    "\n",
    "```py\n",
    "{\"Electronics\": 0.10, \"Furniture\": 0.15, \"Clothing\": 0.05, \"Books\": 0.20}\n",
    "```\n",
    "11. Accumulator: Count Transactions Below 300\n",
    "12. Filter and Save Results\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03-spark-rdd",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
