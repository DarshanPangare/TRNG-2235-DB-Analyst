{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18c82a99-393b-4e5b-8656-f3a5ec519192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark DataFrames API\n",
    "\n",
    "\n",
    "### Dataframe\n",
    "\n",
    "- DataFrames are ditributed collections of records, all with pre-defined structure(schema - structure and data types of all columns)\n",
    "-  DataFrames are built on Spark's core concepts but with structure, optimization and SQL-like operations for data manipulation.\n",
    "- DataFrames track their schema and provide native support for many common SQL functions and relational operators\n",
    "- DataFrames are evaluated as DAGs, using lazy evaluation and providing lineage and fault tolerance.\n",
    "- DataFrames are immutable\n",
    "\n",
    "### SparkContext vs SparkSession\n",
    "\n",
    "- SparkSession is Spark application entry point. \n",
    "- Introduced in spark 2.0 as a unified entry point for all contexts (formerly instantiated individually as SparkContext, SQLContext, HiveContext, StreamingContext)\n",
    "\n",
    "<i>Note: In databricks it is automatically created for you as spark</i>\n",
    "\n",
    "### DataFrame API Optimizations\n",
    "\n",
    "- **Adaptive Query Execution:** Dynamic plan adjustments during runtime based on actual data characteristics and execution patterns.\n",
    "- **In-Memory Columnar Storage(Tungsten):** In-Memory coloumnar format for all the DataFrames enabling efficient analytical query performance and reduced memory footprint.\n",
    "- **Built-in Statistics** - Automatic statistics collection when saving to optimized formats (Parqurt, Delta in databricks) enables smarter query planning and execution.\n",
    "- **Catalyst Optimizer:** Query optimization engine that coverts DataFrame operations into an optimized execution plan\n",
    "\n",
    "\n",
    "<i>**Note** Databricks comes with a native vectorized query engine that accelerates query execution using photon engine</i>\n",
    "\n",
    "**DataFrame Query Planning:** \n",
    "\n",
    "- When a DataFrame is evaluated, the driver creates an optimized execution plan through a series of transformations \n",
    "- Converts the logical plan into phycal execution that minimizes resource usage and execution time. (Unresolved LP -> analysed LP -> optimized LP -> Physical Plan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13d5fc4-a0be-45b8-a1a8-1fe86e88e239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "726916e6-0294-4687-b8c5-5823798112de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating DataFrames - DataFrameReader\n",
    "# supports multiple formats such as JSON, CSV, Parquet, ORC, Text or Binary files, existing RDD, and an external db\n",
    "\n",
    "df_customers = spark.read.csv(\"/Volumes/workspace/2235_wk1/retail-data/raw_customer_data.csv\", header=True, inferSchema=True, escape='\"')\n",
    "\n",
    "df_customers.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7406575f-8750-452e-926d-babb32379f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DataFrame Data Types\n",
    "\n",
    "#### Primitive\n",
    "\n",
    "**`pyspark.sql.types.DataType`**\n",
    "\n",
    "- `ByteType`\n",
    "- `ShortType`\n",
    "- `IntegerType`\n",
    "- `LongType`\n",
    "- `FloatType`\n",
    "- `DoubleType`\n",
    "- `BooleanType`\n",
    "- `StringType`\n",
    "- `BinaryType`\n",
    "- `TimestampType`\n",
    "- `DateType`\n",
    "\n",
    "#### complex data types\n",
    "\n",
    "- `ArrayType`\n",
    "- `MapType`\n",
    "- `StructType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56fa3ad4-d7cd-48a1-b2c5-a29b927ffb69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5f3dbe-7953-4851-951d-d8be62589bb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d7b3c54-7108-4391-846e-880a5d93d223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Schema\n",
    "# custom schema definition\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, BooleanType, TimestampType, DateType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"age\", DoubleType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"signup_date\", DateType(), True),\n",
    "    StructField(\"last_login\", TimestampType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True),\n",
    "    StructField(\"total_spent\", DoubleType(), True),\n",
    "    StructField(\"preferences\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_customers = spark.read.csv(\"/Volumes/workspace/2235_wk1/retail-data/raw_customer_data.csv\", header=True, schema=custom_schema,  escape='\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0fea9de-a0f3-486a-8bcd-0202908845eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DDL Schema\n",
    "\n",
    "ddl_schema = \"\"\"\n",
    "customer_id STRING,\n",
    "name STRING,\n",
    "email STRING,\n",
    "age DOUBLE,\n",
    "gender STRING,\n",
    "country STRING,\n",
    "signup_date DATE,\n",
    "last_login TIMESTAMP,\n",
    "is_active BOOLEAN,\n",
    "total_spent DOUBLE,\n",
    "preferences STRING\n",
    "\"\"\"\n",
    "\n",
    "df_customers = spark.read.csv(\"/Volumes/workspace/2235_wk1/retail-data/raw_customer_data.csv\", header=True, schema=ddl_schema,  escape='\"')\n",
    "\n",
    "df_customers.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb219e89-64de-4af7-b998-6906d4071e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Common DataFrame API methods\n",
    "\n",
    "#### Transformations\n",
    "\n",
    "##### Narrow Transformations\n",
    "\n",
    "- narrow transformations process data within each partition independetly, without needing to combine data from other partitions.\n",
    "- faster and more efficient because they avoid data shuffling between partitions. \n",
    "\n",
    "1. `select()` : selecting specific rows\n",
    "2. `filter()`: Applying a filter condition to rows. \n",
    "3. `map()`: Applying a function to each row. \n",
    "4. `union()`: Combining two DataFrames with identical schemas. \n",
    "5. `withColumn()`: Adding a new column based on existing ones. \n",
    "6. `drop()`: Removing a column. \n",
    "\n",
    "##### Wide Transformations\n",
    "\n",
    "- Wide transformations require data to be redistributed across partitions, often involving shuffling data based on keys.\n",
    "\n",
    "1. `groupBy()`: Grouping data based on a column, which often requires shuffling to aggregate data from different partitions. \n",
    "2. `join()`: Joining two DataFrames, which requires shuffling data to combine rows based on a join key. \n",
    "3. `distinct()`: Removing duplicate rows, which might require shuffling to compare rows across partitions. \n",
    "\n",
    "#### Actions\n",
    "\n",
    "1. `count()`: returns number of rows in a Dataframe\n",
    "2. `show()`: display DataFrame content\n",
    "3. `take(n)`: return first n rows from a DataFrame\n",
    "4. `first()`: return first row from a DataFrame\n",
    "5. `write()`: save DataFrame to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df45cad7-a434-48b9-9ca0-9ec0522a9a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350a1de5-384e-417c-a632-a192d9a02b39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Map, Shuffle and Reduce\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df_customers.filter(df_customers.age > 30) \\\n",
    "    .select(\"country\", \"total_spent\") \\\n",
    "        .groupBy(\"country\") \\\n",
    "            .agg(sum(\"total_spent\").alias(\"revenue\")).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "728ae25e-3955-482f-932d-67dd2497040e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Handiling missing values\n",
    "\n",
    "**common functions**\n",
    "\n",
    "- `isNull()`/`isNotNull()` - checks if values are null\n",
    "- `count(col)` - counts non null values in a specific column\n",
    "- `df.fillna()`/`df.na.fill()` - replace nulls with values\n",
    "- `df.dropna()`/`df.na.drop()` - remove rows with nulls\n",
    "\n",
    "#### referencing columns\n",
    "\n",
    "- direct - `df.select(\"col_name\")` - basic column selection\n",
    "\n",
    "- by attribute - `df.select(df.attribute)` - column names that are valid python idenfiers, can be referenced across DataFrames(e.g., join)\n",
    "\n",
    "- column expression - `df.select(df[\"col_name\"])` - any column names, can be referenced across DataFrames\n",
    "\n",
    "- column object - `df.select(col(\"name\"))` - required when building complex expressions or using column specific operations like `cast()`, `alias()`, `asc()`or `desc()`\n",
    "\n",
    "\n",
    "#### common column object methods\n",
    "\n",
    "- `alias()` - rename column\n",
    "- `cast()` - chnage data type\n",
    "- `isNull()` or `isNotNull()` - check for nulls\n",
    "- `contains()` - string matching\n",
    "- `asc()`/`desc()` - sort direction - `df.sort(col(\"c1\").asc())`\n",
    "\n",
    "\n",
    "[pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.html)\n",
    "\n",
    "#### common built-in functions\n",
    "\n",
    "- `concat(col1, col2)` - concatenate strings\n",
    "- `date_format(col, fmt)` - fromat date strings\n",
    "- `round(col, scale)` - round number to scale\n",
    "- `regexp_replace(col, pattern, replace)` - replace using regex\n",
    "- `coalesce(col1, col2)` - first non null value\n",
    "\n",
    "\n",
    "[pyspark built in functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "\n",
    "#### user defined functions\n",
    "\n",
    "- allows to use python functions on dataframe columns\n",
    "- helpful to create custom reusable functions\n",
    "- can impact performance as they can not be optimized by the Catalyst optimizer and has serialiation overhead.\n",
    "\n",
    "```py\n",
    "@udf(\"data_type\")\n",
    "def function_name(name):\n",
    "    return value\n",
    "\n",
    "df.select(function_name(\"name\"))\n",
    "```\n",
    "\n",
    "<i><b>note:</b> pandas udfs are efficient becuase they operate on batches of raows instead of single rows, they leverage Apache Arrow for more efficient python-JVM serialization </i>\n",
    "\n",
    "### Aggregate functions\n",
    "\n",
    "can be applied in `agg()` method after `groupBy()` operation or directly within `select()`\n",
    "\n",
    "- `sum()`\n",
    "- `avg()`\n",
    "- `min()`\n",
    "- `max()`\n",
    "- `count()`\n",
    "- `first()`\n",
    "- `last()`\n",
    "\n",
    "### Spark SQL\n",
    "\n",
    "- Spark SQL is a module in spark that allows you to run SQL queries on structured and semi-structured data.\n",
    "- It provides a SQL-like interface on top of Spark's powerful DataFrame API, enabling both SQL and programmatic access to data, all with the same optimized execution engine.\n",
    "\n",
    "- `createOrReplaceTempView()` : created a temp in-memory view that you can query with SQL.\n",
    "- `createGlobalTempView()` : Creates a global temporary view accessible across sessions (under global_temp database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6184651c-ef51-44c1-84b9-877a29c9ee11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42d52e90-e59b-4910-b053-d9395ad4ca32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58cd9b28-d5ca-4710-8ed3-f6cdea97176b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop  records with missing email\n",
    "df_clean = df_customers.na.drop(subset=\"email\")\n",
    "\n",
    "df_clean.filter(col(\"email\").isNull()==True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc765d8-1e4d-46f7-a64a-a3cddca13d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df_clean.na.fill({\"gender\": \"Unknown\"})\n",
    "\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a806d2c3-230f-4b91-bea9-f4bb0dcb4046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filling missing age with median age\n",
    "\n",
    "median_age = df_clean.approxQuantile(\"age\", [0.5], 0.01)[0]\n",
    "\n",
    "df_clean = df_clean.na.fill({\"age\": median_age})\n",
    "\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a92c10a-5419-4cbe-bdbb-eda91dfb8398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# remove zero and negative total_spent\n",
    "\n",
    "df_clean = df_clean.filter((col(\"total_spent\").isNotNull()) & (col(\"total_spent\") > 0))\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94c3085a-c698-4499-b623-618d84923bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc4c5549-51d4-40cb-8aef-b49ed9b5ee5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_enriched = df_clean.withColumn(\"age_group\", \n",
    "                                  when(col(\"age\")<30, \"Young\")\n",
    "                                  .when((col(\"age\")>=30) & (col(\"age\")<50), \"Adult\")\n",
    "                                  .when(col(\"age\")>=50, \"Senior\")\n",
    ")\n",
    "\n",
    "df_enriched.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af79025-3408-475c-9310-54d48cef40d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parse preferences JSON column to struct\n",
    "\n",
    "df_enriched.select(\"preferences\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c0ae83-dc7a-4d63-8ff2-2a895db9977c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preference_schema = StructType([\n",
    "    StructField(\"newsletter\", BooleanType(), True),\n",
    "    StructField(\"notifications\", StringType(), True),\n",
    "    StructField(\"language\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_parsed = df_enriched.withColumn(\"preferences_stuct\", from_json(col(\"preferences\"), preference_schema))\n",
    "\n",
    "df_parsed.select(\"preferences_stuct\").show(truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ddbe54d-7313-4fb8-8000-99755e64e75b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_parsed \\\n",
    "    .withColumn(\"pref_newsletter\", col(\"preferences_stuct.newsletter\")) \\\n",
    "    .withColumn(\"pref_notifications\", col(\"preferences_stuct.notifications\")) \\\n",
    "    .withColumn(\"pref_language\", col(\"preferences_stuct.language\")) \\\n",
    "    .drop(\"preferences_stuct\", \"preferences\")\n",
    "\n",
    "df_final.select(\"customer_id\", \"pref_newsletter\",\"pref_notifications\",\"pref_language\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21905f49-d056-41dc-83ee-ee0e71ce05a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# custom udf to split first_name and last_name\n",
    "\n",
    "return_type = StructType([\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "@udf(return_type)\n",
    "def split_full_name(full_name:str) -> dict:\n",
    "    if full_name is None:\n",
    "        return None\n",
    "    parts = full_name.split(\" \")\n",
    "    first_name = parts[0]\n",
    "    if(len(parts)>1):\n",
    "        last_name = parts[1]\n",
    "    else:\n",
    "        last_name = None\n",
    "    return {\"first_name\": first_name, \"last_name\": last_name}\n",
    "\n",
    "df_final_with_split_name = df_final.withColumn(\"name_parts\", split_full_name(col(\"name\")))\n",
    "\n",
    "df_final_with_split_name.select(\"name\", \"name_parts\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c06735f-92ea-4837-94fc-0b55253eddb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_final_with_split_name.withColumn(\"first_name\", col(\"name_parts.first_name\")) \\\n",
    "                                  .withColumn(\"last_name\", col(\"name_parts.last_name\")) \\\n",
    "                                      .drop(\"name_parts\", \"name\")\n",
    "                                        \n",
    "df_final.select(\"customer_id\", \"first_name\", \"last_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50b440fd-0031-4c68-8507-bdd753d00581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5625a6a8-c4d3-4e90-b09d-d1d276a767c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# total and average revenue by country\n",
    "\n",
    "df_final.groupBy(\"country\").agg(\n",
    "    count(\"*\").alias(\"customers\"),\n",
    "    sum(\"total_spent\").alias(\"total_revenue\"),\n",
    "    round(avg(\"total_spent\"),2).alias(\"avg_revenue\")\n",
    "    ).orderBy(col(\"total_revenue\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d020ce8-9dce-4123-a88b-0654dfb294e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save to parquet\n",
    "\n",
    "df_final.write.mode(\"overwrite\").parquet(\"/Volumes/workspace/2235_wk1/retail-data/final_customers.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e1674bb-8df7-4765-b51c-8e2c66586055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Compare Spark SQL and DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6335edd6-60e5-47ec-a65e-b457285d4a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.createOrReplaceGlobalTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e00a11-9e68-4381-9516-3ec175db7ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql_plan = spark.sql(\"\"\" SELECT country, COUNT(*) AS total, ROUND(AVG(total_spent),2) AS avg_spent\n",
    "          FROM customers\n",
    "          GROUP BY country\n",
    "          \"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b25db7-4746-4949-9adf-56b6d01b90cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_plan = df_final.groupBy(\"country\").agg(\n",
    "    count(\"*\").alias(\"total\"),\n",
    "    round(avg(\"total_spent\"),2).alias(\"avg_spent\")\n",
    ").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c28ebc2e-a9cd-4fcc-959f-f85c55770f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql_plan==df_plan"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03-spark-dataframes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
