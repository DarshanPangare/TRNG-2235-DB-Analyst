{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb2bcbf3-dfb9-47e1-b55c-e63644ca162d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# RDD\n",
    "\n",
    "an RDD is the fundamental data structure of Apache Spark. It's a fault-tolerant, distributed collection of elements that can be operated on in parallel.\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "- Immutable\n",
    "- Lazy evaluation\n",
    "- Fault tolerant (via lineage info)\n",
    "- Partitioned across cluster nodes\n",
    "- Can be cached in memory\n",
    "\n",
    "### SparkContext and SparkConf\n",
    "\n",
    "\n",
    "SparkContext is the entry point for Spark functionality.\n",
    "\n",
    "#### `SparkConf`\n",
    "\n",
    "- Configuration for Spark application\n",
    "\n",
    "**Common settings:**\n",
    "\n",
    "- setMaster(\"local[*]\") – Use local mode with all cores\n",
    "- setAppName(\"RDDExample\") – Application name\n",
    "\n",
    "e.g.,\n",
    "\n",
    "\n",
    "### transformations\n",
    "\n",
    "Transformations create a new RDD from an existing one. They are lazy – not executed until an action is triggered.\n",
    "\n",
    "| Transformation  | Description                                          |\n",
    "| --------------- | ---------------------------------------------------- |\n",
    "| `map(func)`     | Returns a new RDD by applying `func` to each element |\n",
    "| `filter(func)`  | Filters elements for which `func` returns true       |\n",
    "| `flatMap(func)` | Like map but flattens the result                     |\n",
    "| `distinct()`    | Removes duplicates                                   |\n",
    "| `union(rdd)`    | Combines two RDDs                                    |\n",
    "| `groupByKey()`  | Groups values with same key                          |\n",
    "| `reduceByKey()` | Aggregates values with same key using a function     |\n",
    "| `sortBy(func)`  | Sorts RDD by computed key                            |\n",
    "\n",
    "\n",
    "### actions\n",
    "\n",
    "Actions trigger computation and return results or write data.\n",
    "\n",
    "| Action             | Description                            |\n",
    "| ------------------ | -------------------------------------- |\n",
    "| `collect()`        | Returns all elements to driver         |\n",
    "| `count()`          | Returns number of elements             |\n",
    "| `first()`          | Returns first element                  |\n",
    "| `take(n)`          | Returns first `n` elements             |\n",
    "| `reduce(func)`     | Reduces elements using binary operator |\n",
    "| `saveAsTextFile()` | Writes RDD to text files               |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reference - [spark rdd docs](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4be44b60-d539-48c8-b775-f83ece7949c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pyspark) (0.10.9.9)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark\n",
    "\n",
    "# pyspark 4.0.0 - jdk 17 or above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "224bf95a-5ee9-4210-9c1b-0104d7a3cb45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "! pip install pyspark==3.5.0\n",
    "\n",
    "# pyspark 3.5 - jdk 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "353b4018-e10e-4d11-b3b6-25185519ed72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/29 19:44:01 WARN Utils: Your hostname, codespaces-b78a22, resolves to a loopback address: 127.0.0.1; using 10.0.2.187 instead (on interface eth0)\n",
      "25/07/29 19:44:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/29 19:44:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"regionSalesDemo\").setMaster(\"local[*]\")\n",
    "\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ac24ef9e-b3a5-4839-adc4-181fe6faed47.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>regionSalesDemo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=regionSalesDemo>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_raw = sc.textFile(\"file:///workspaces/TRNG-2235-DB-Analyst/week1/datasets/region_sales_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2001,South,Electronics,838',\n",
       " '2002,West,Furniture,471',\n",
       " '2003,North,Electronics,803',\n",
       " '2004,West,Furniture,174',\n",
       " '2005,South,Clothing,590',\n",
       " '2006,North,Furniture,937',\n",
       " '2007,North,Electronics,391',\n",
       " '2008,West,Electronics,961',\n",
       " '2009,North,Electronics,305',\n",
       " '2010,East,Clothing,213',\n",
       " '2011,East,Electronics,615',\n",
       " '2012,South,Clothing,573',\n",
       " '2013,East,Clothing,352',\n",
       " '2014,West,Clothing,768',\n",
       " '2015,North,Electronics,231',\n",
       " '2016,East,Furniture,217',\n",
       " '2017,West,Clothing,346',\n",
       " '2018,North,Books,375',\n",
       " '2019,South,Electronics,313',\n",
       " '2020,West,Furniture,903',\n",
       " '2021,West,Clothing,904',\n",
       " '2022,North,Furniture,812',\n",
       " '2023,West,Furniture,590',\n",
       " '2024,North,Books,452',\n",
       " '2025,North,Books,697',\n",
       " '2026,East,Clothing,959',\n",
       " '2027,North,Books,661',\n",
       " '2028,South,Books,700',\n",
       " '2029,East,Clothing,832',\n",
       " '2030,North,Furniture,163',\n",
       " '2031,South,Books,413',\n",
       " '2032,North,Furniture,952',\n",
       " '2033,North,Books,240',\n",
       " '2034,East,Clothing,740',\n",
       " '2035,South,Clothing,264',\n",
       " '2036,West,Books,211',\n",
       " '2037,East,Clothing,103',\n",
       " '2038,North,Electronics,462',\n",
       " '2039,West,Electronics,479',\n",
       " '2040,East,Electronics,666',\n",
       " '2041,West,Clothing,347',\n",
       " '2042,South,Electronics,435',\n",
       " '2043,East,Clothing,669',\n",
       " '2044,West,Electronics,715',\n",
       " '2045,North,Electronics,232',\n",
       " '2046,South,Electronics,569',\n",
       " '2047,West,Books,989',\n",
       " '2048,North,Clothing,507',\n",
       " '2049,East,Clothing,429',\n",
       " '2050,West,Electronics,679',\n",
       " '2051,West,Books,581',\n",
       " '2052,West,Furniture,770',\n",
       " '2053,South,Furniture,408',\n",
       " '2054,East,Furniture,111',\n",
       " '2055,South,Electronics,228',\n",
       " '2056,North,Electronics,430',\n",
       " '2057,South,Furniture,523',\n",
       " '2058,South,Books,599',\n",
       " '2059,East,Clothing,756',\n",
       " '2060,South,Electronics,937',\n",
       " '2061,East,Electronics,456',\n",
       " '2062,North,Books,238',\n",
       " '2063,West,Electronics,210',\n",
       " '2064,North,Electronics,922',\n",
       " '2065,North,Books,113',\n",
       " '2066,North,Furniture,700',\n",
       " '2067,South,Furniture,643',\n",
       " '2068,South,Clothing,427',\n",
       " '2069,West,Electronics,909',\n",
       " '2070,West,Electronics,818',\n",
       " '2071,North,Clothing,438',\n",
       " '2072,East,Books,627',\n",
       " '2073,East,Furniture,303',\n",
       " '2074,South,Clothing,1000',\n",
       " '2075,South,Electronics,453',\n",
       " '2076,East,Books,461',\n",
       " '2077,South,Books,217',\n",
       " '2078,South,Clothing,122',\n",
       " '2079,North,Clothing,761',\n",
       " '2080,East,Books,783',\n",
       " '2081,North,Clothing,135',\n",
       " '2082,North,Books,412',\n",
       " '2083,West,Clothing,336',\n",
       " '2084,West,Books,129',\n",
       " '2085,South,Clothing,203',\n",
       " '2086,South,Furniture,856',\n",
       " '2087,West,Books,507',\n",
       " '2088,East,Furniture,681',\n",
       " '2089,South,Furniture,887',\n",
       " '2090,West,Electronics,295',\n",
       " '2091,North,Furniture,777',\n",
       " '2092,South,Clothing,913',\n",
       " '2093,South,Books,535',\n",
       " '2094,North,Electronics,946',\n",
       " '2095,West,Clothing,382',\n",
       " '2096,North,Electronics,295',\n",
       " '2097,West,Furniture,402',\n",
       " '2098,South,Furniture,552',\n",
       " '2099,South,Clothing,341',\n",
       " '2100,West,Books,799']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_raw.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2001', 'South', 'Electronics', '838'],\n",
       " ['2002', 'West', 'Furniture', '471'],\n",
       " ['2003', 'North', 'Electronics', '803'],\n",
       " ['2004', 'West', 'Furniture', '174']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = sales_raw.map(lambda x:x.split(\",\"))\n",
    "\n",
    "records.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2001, 'South', 'Electronics', 838),\n",
       " (2002, 'West', 'Furniture', 471),\n",
       " (2003, 'North', 'Electronics', 803),\n",
       " (2004, 'West', 'Furniture', 174),\n",
       " (2005, 'South', 'Clothing', 590),\n",
       " (2006, 'North', 'Furniture', 937),\n",
       " (2007, 'North', 'Electronics', 391),\n",
       " (2008, 'West', 'Electronics', 961),\n",
       " (2009, 'North', 'Electronics', 305)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = records.map(lambda x:(int(x[0]), x[1], x[2], int(x[3])))\n",
    "records.take(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('South', 123), ('Ele', 123), ('West', 345), ('Furniture', 345)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optional rdd- list\n",
    "rdd = sc.parallelize([\n",
    "    \"2001,South,Ele,123\",\n",
    "    \"2002,West,Furniture,345\"\n",
    "])\n",
    "\n",
    "pairs = rdd.map(lambda line:[\n",
    "    (line.split(\",\")[1], int(line.split(\",\")[3])),\n",
    "    (line.split(\",\")[2], int(line.split(\",\")[3])),\n",
    "])\n",
    "\n",
    "pairs = pairs.flatMap(lambda x: x)\n",
    "\n",
    "\n",
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SouthElectronics', 838),\n",
       " ('WestFurniture', 471),\n",
       " ('NorthElectronics', 803),\n",
       " ('WestFurniture', 174),\n",
       " ('SouthClothing', 590),\n",
       " ('NorthFurniture', 937),\n",
       " ('NorthElectronics', 391),\n",
       " ('WestElectronics', 961),\n",
       " ('NorthElectronics', 305),\n",
       " ('EastClothing', 213),\n",
       " ('EastElectronics', 615),\n",
       " ('SouthClothing', 573),\n",
       " ('EastClothing', 352),\n",
       " ('WestClothing', 768),\n",
       " ('NorthElectronics', 231),\n",
       " ('EastFurniture', 217),\n",
       " ('WestClothing', 346),\n",
       " ('NorthBooks', 375),\n",
       " ('SouthElectronics', 313),\n",
       " ('WestFurniture', 903),\n",
       " ('WestClothing', 904),\n",
       " ('NorthFurniture', 812),\n",
       " ('WestFurniture', 590),\n",
       " ('NorthBooks', 452),\n",
       " ('NorthBooks', 697),\n",
       " ('EastClothing', 959),\n",
       " ('NorthBooks', 661),\n",
       " ('SouthBooks', 700),\n",
       " ('EastClothing', 832),\n",
       " ('NorthFurniture', 163),\n",
       " ('SouthBooks', 413),\n",
       " ('NorthFurniture', 952),\n",
       " ('NorthBooks', 240),\n",
       " ('EastClothing', 740),\n",
       " ('SouthClothing', 264),\n",
       " ('WestBooks', 211),\n",
       " ('EastClothing', 103),\n",
       " ('NorthElectronics', 462),\n",
       " ('WestElectronics', 479),\n",
       " ('EastElectronics', 666),\n",
       " ('WestClothing', 347),\n",
       " ('SouthElectronics', 435),\n",
       " ('EastClothing', 669),\n",
       " ('WestElectronics', 715),\n",
       " ('NorthElectronics', 232),\n",
       " ('SouthElectronics', 569),\n",
       " ('WestBooks', 989),\n",
       " ('NorthClothing', 507),\n",
       " ('EastClothing', 429),\n",
       " ('WestElectronics', 679),\n",
       " ('WestBooks', 581),\n",
       " ('WestFurniture', 770),\n",
       " ('SouthFurniture', 408),\n",
       " ('EastFurniture', 111),\n",
       " ('SouthElectronics', 228),\n",
       " ('NorthElectronics', 430),\n",
       " ('SouthFurniture', 523),\n",
       " ('SouthBooks', 599),\n",
       " ('EastClothing', 756),\n",
       " ('SouthElectronics', 937),\n",
       " ('EastElectronics', 456),\n",
       " ('NorthBooks', 238),\n",
       " ('WestElectronics', 210),\n",
       " ('NorthElectronics', 922),\n",
       " ('NorthBooks', 113),\n",
       " ('NorthFurniture', 700),\n",
       " ('SouthFurniture', 643),\n",
       " ('SouthClothing', 427),\n",
       " ('WestElectronics', 909),\n",
       " ('WestElectronics', 818),\n",
       " ('NorthClothing', 438),\n",
       " ('EastBooks', 627),\n",
       " ('EastFurniture', 303),\n",
       " ('SouthClothing', 1000),\n",
       " ('SouthElectronics', 453),\n",
       " ('EastBooks', 461),\n",
       " ('SouthBooks', 217),\n",
       " ('SouthClothing', 122),\n",
       " ('NorthClothing', 761),\n",
       " ('EastBooks', 783),\n",
       " ('NorthClothing', 135),\n",
       " ('NorthBooks', 412),\n",
       " ('WestClothing', 336),\n",
       " ('WestBooks', 129),\n",
       " ('SouthClothing', 203),\n",
       " ('SouthFurniture', 856),\n",
       " ('WestBooks', 507),\n",
       " ('EastFurniture', 681),\n",
       " ('SouthFurniture', 887),\n",
       " ('WestElectronics', 295),\n",
       " ('NorthFurniture', 777),\n",
       " ('SouthClothing', 913),\n",
       " ('SouthBooks', 535),\n",
       " ('NorthElectronics', 946),\n",
       " ('WestClothing', 382),\n",
       " ('NorthElectronics', 295),\n",
       " ('WestFurniture', 402),\n",
       " ('SouthFurniture', 552),\n",
       " ('SouthClothing', 341),\n",
       " ('WestBooks', 799)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total sales by category and region\n",
    "\n",
    "category_sales = records.map(lambda x:((x[1]+x[2]), x[3]))\n",
    "category_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Electronics', 838),\n",
       " ('Furniture', 471),\n",
       " ('Electronics', 803),\n",
       " ('Furniture', 174),\n",
       " ('Clothing', 590),\n",
       " ('Furniture', 937),\n",
       " ('Electronics', 391),\n",
       " ('Electronics', 961),\n",
       " ('Electronics', 305),\n",
       " ('Clothing', 213),\n",
       " ('Electronics', 615),\n",
       " ('Clothing', 573),\n",
       " ('Clothing', 352),\n",
       " ('Clothing', 768),\n",
       " ('Electronics', 231),\n",
       " ('Furniture', 217),\n",
       " ('Clothing', 346),\n",
       " ('Books', 375),\n",
       " ('Electronics', 313),\n",
       " ('Furniture', 903),\n",
       " ('Clothing', 904),\n",
       " ('Furniture', 812),\n",
       " ('Furniture', 590),\n",
       " ('Books', 452),\n",
       " ('Books', 697),\n",
       " ('Clothing', 959),\n",
       " ('Books', 661),\n",
       " ('Books', 700),\n",
       " ('Clothing', 832),\n",
       " ('Furniture', 163),\n",
       " ('Books', 413),\n",
       " ('Furniture', 952),\n",
       " ('Books', 240),\n",
       " ('Clothing', 740),\n",
       " ('Clothing', 264),\n",
       " ('Books', 211),\n",
       " ('Clothing', 103),\n",
       " ('Electronics', 462),\n",
       " ('Electronics', 479),\n",
       " ('Electronics', 666),\n",
       " ('Clothing', 347),\n",
       " ('Electronics', 435),\n",
       " ('Clothing', 669),\n",
       " ('Electronics', 715),\n",
       " ('Electronics', 232),\n",
       " ('Electronics', 569),\n",
       " ('Books', 989),\n",
       " ('Clothing', 507),\n",
       " ('Clothing', 429),\n",
       " ('Electronics', 679),\n",
       " ('Books', 581),\n",
       " ('Furniture', 770),\n",
       " ('Furniture', 408),\n",
       " ('Furniture', 111),\n",
       " ('Electronics', 228),\n",
       " ('Electronics', 430),\n",
       " ('Furniture', 523),\n",
       " ('Books', 599),\n",
       " ('Clothing', 756),\n",
       " ('Electronics', 937),\n",
       " ('Electronics', 456),\n",
       " ('Books', 238),\n",
       " ('Electronics', 210),\n",
       " ('Electronics', 922),\n",
       " ('Books', 113),\n",
       " ('Furniture', 700),\n",
       " ('Furniture', 643),\n",
       " ('Clothing', 427),\n",
       " ('Electronics', 909),\n",
       " ('Electronics', 818),\n",
       " ('Clothing', 438),\n",
       " ('Books', 627),\n",
       " ('Furniture', 303),\n",
       " ('Clothing', 1000),\n",
       " ('Electronics', 453),\n",
       " ('Books', 461),\n",
       " ('Books', 217),\n",
       " ('Clothing', 122),\n",
       " ('Clothing', 761),\n",
       " ('Books', 783),\n",
       " ('Clothing', 135),\n",
       " ('Books', 412),\n",
       " ('Clothing', 336),\n",
       " ('Books', 129),\n",
       " ('Clothing', 203),\n",
       " ('Furniture', 856),\n",
       " ('Books', 507),\n",
       " ('Furniture', 681),\n",
       " ('Furniture', 887),\n",
       " ('Electronics', 295),\n",
       " ('Furniture', 777),\n",
       " ('Clothing', 913),\n",
       " ('Books', 535),\n",
       " ('Electronics', 946),\n",
       " ('Clothing', 382),\n",
       " ('Electronics', 295),\n",
       " ('Furniture', 402),\n",
       " ('Furniture', 552),\n",
       " ('Clothing', 341),\n",
       " ('Books', 799)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total sales by category\n",
    "\n",
    "category_sales = records.map(lambda x:(x[2], x[3]))\n",
    "category_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales_by_category = category_sales.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Furniture', 12832),\n",
       " ('Clothing', 14410),\n",
       " ('Electronics', 15593),\n",
       " ('Books', 10739)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sales_by_category.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Furniture', 583.2727272727273),\n",
       " ('Clothing', 514.6428571428571),\n",
       " ('Electronics', 556.8928571428571),\n",
       " ('Books', 488.1363636363636)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average sales per category\n",
    "\n",
    "average_sales_by_category = category_sales.mapValues(lambda x:(x,1)) \\\n",
    "                                .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "                                .mapValues(lambda x: x[0]/x[1])\n",
    "\n",
    "average_sales_by_category.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Electronics', 15593)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# highgest selling category\n",
    "\n",
    "highest_selling_category = total_sales_by_category.max(key=lambda x: x[1])\n",
    "\n",
    "highest_selling_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Furniture', 12832), ('Clothing', 14410), ('Electronics', 15593)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categories with sales above 12000\n",
    "\n",
    "high_selling_cat_12k = total_sales_by_category.filter(lambda x: x[1]>12000)\n",
    "\n",
    "high_selling_cat_12k.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e9d9b73-12ae-41b9-bf9b-b9e68669c128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Shared Variables\n",
    "\n",
    "When you pass a function (like in map or reduce) to Spark, that function runs on different machines in the cluster — not on your local driver.\n",
    "\n",
    "**By default:**\n",
    "\n",
    "- Spark makes separate copies of any variable you use inside those functions.\n",
    "- So, if a task changes a variable on the executor, that change will not reflect back in your driver program.\n",
    "\n",
    "This is done to keep things fast and distributed — but it also means you can not just update normal variables across tasks.\n",
    "\n",
    "**challenges:**\n",
    "\n",
    "You want to count how many rows have Amount > 500 using this code:\n",
    "\n",
    "```py\n",
    "count = 0\n",
    "\n",
    "def increment_count(x):\n",
    "    count += 1 \n",
    "\n",
    "records.filter(lambda x: int(x[3]) > 500).foreach(increment_count)\n",
    "print(count)\n",
    "\n",
    "```\n",
    "\n",
    "This will not work because each machine updates its own copy of count not the original one in the driver.\n",
    "\n",
    "To solve such issues, spark provides two types of shared variables:\n",
    "\n",
    "1. **Broadcast Variables:** a read-only variable that can be cached on each machine (executor). Used to efficiently share large data (like lookup tables) with all tasks without copying it multiple times.\n",
    "2. **Accumulators:** variables used to safely implement counters or sums across mulitple worker nodes. You can only add to them (not read or subtract inside tasks). The final value is only accessible on the driver after an action is executed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/29 19:56:19 ERROR Executor: Exception in task 1.0 in stage 42.0 (TID 62)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2044, in main\n",
      "    process()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2034, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 705, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 1630, in processPartition\n",
      "    f(x)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/util.py\", line 131, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_38434/1037894068.py\", line 4, in increment_count\n",
      "UnboundLocalError: cannot access local variable 'count' where it is not associated with a value\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:940)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/07/29 19:56:19 WARN TaskSetManager: Lost task 1.0 in stage 42.0 (TID 62) (ac24ef9e-b3a5-4839-adc4-181fe6faed47.internal.cloudapp.net executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2044, in main\n",
      "    process()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2034, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 705, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 1630, in processPartition\n",
      "    f(x)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/util.py\", line 131, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_38434/1037894068.py\", line 4, in increment_count\n",
      "UnboundLocalError: cannot access local variable 'count' where it is not associated with a value\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:940)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/07/29 19:56:19 ERROR TaskSetManager: Task 1 in stage 42.0 failed 1 times; aborting job\n",
      "25/07/29 19:56:19 WARN TaskSetManager: Lost task 0.0 in stage 42.0 (TID 61) (ac24ef9e-b3a5-4839-adc4-181fe6faed47.internal.cloudapp.net executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 42.0 failed 1 times, most recent failure: Lost task 1.0 in stage 42.0 (TID 62) (ac24ef9e-b3a5-4839-adc4-181fe6faed47.internal.cloudapp.net executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2044, in main\n",
      "    process()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2034, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 705, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 1630, in processPartition\n",
      "    f(x)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/util.py\", line 131, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_38434/1037894068.py\", line 4, in increment_count\n",
      "UnboundLocalError: cannot access local variable 'count' where it is not associated with a value\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:940)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 42.0 failed 1 times, most recent failure: Lost task 1.0 in stage 42.0 (TID 62) (ac24ef9e-b3a5-4839-adc4-181fe6faed47.internal.cloudapp.net executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2044, in main\n    process()\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2034, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  [Previous line repeated 1 more time]\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 705, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 1630, in processPartition\n    f(x)\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/util.py\", line 131, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_38434/1037894068.py\", line 4, in increment_count\nUnboundLocalError: cannot access local variable 'count' where it is not associated with a value\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:940)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2044, in main\n    process()\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2034, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  [Previous line repeated 1 more time]\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 705, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 1630, in processPartition\n    f(x)\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/util.py\", line 131, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_38434/1037894068.py\", line 4, in increment_count\nUnboundLocalError: cannot access local variable 'count' where it is not associated with a value\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:940)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mincrement_count\u001b[39m(x):\n\u001b[32m      4\u001b[39m     count += \u001b[32m1\u001b[39m \n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mrecords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m(\u001b[49m\u001b[43mincrement_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py:1633\u001b[39m, in \u001b[36mRDD.foreach\u001b[39m\u001b[34m(self, f)\u001b[39m\n\u001b[32m   1630\u001b[39m         f(x)\n\u001b[32m   1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m([])\n\u001b[32m-> \u001b[39m\u001b[32m1633\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessPartition\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py:2183\u001b[39m, in \u001b[36mRDD.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   2163\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2164\u001b[39m \u001b[33;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[32m   2165\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2181\u001b[39m \u001b[33;03m    3\u001b[39;00m\n\u001b[32m   2182\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py:2158\u001b[39m, in \u001b[36mRDD.sum\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2137\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msum\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRDD[NumberOrArray]\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mNumberOrArray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2138\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2139\u001b[39m \u001b[33;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[32m   2140\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2156\u001b[39m \u001b[33;03m    6.0\u001b[39;00m\n\u001b[32m   2157\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[32m   2159\u001b[39m \u001b[43m        \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\n\u001b[32m   2160\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py:1911\u001b[39m, in \u001b[36mRDD.fold\u001b[39m\u001b[34m(self, zeroValue, op)\u001b[39m\n\u001b[32m   1906\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[32m   1908\u001b[39m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[32m   1909\u001b[39m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[32m   1910\u001b[39m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m vals = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py:1700\u001b[39m, in \u001b[36mRDD.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m.context):\n\u001b[32m   1699\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1700\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 42.0 failed 1 times, most recent failure: Lost task 1.0 in stage 42.0 (TID 62) (ac24ef9e-b3a5-4839-adc4-181fe6faed47.internal.cloudapp.net executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2044, in main\n    process()\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2034, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  [Previous line repeated 1 more time]\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 705, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 1630, in processPartition\n    f(x)\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/util.py\", line 131, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_38434/1037894068.py\", line 4, in increment_count\nUnboundLocalError: cannot access local variable 'count' where it is not associated with a value\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:940)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2044, in main\n    process()\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2034, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  [Previous line repeated 1 more time]\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 705, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/core/rdd.py\", line 1630, in processPartition\n    f(x)\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/util.py\", line 131, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_38434/1037894068.py\", line 4, in increment_count\nUnboundLocalError: cannot access local variable 'count' where it is not associated with a value\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:940)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "def increment_count(x):\n",
    "    count += 1 \n",
    "\n",
    "records.filter(lambda x: int(x[3]) > 500).foreach(increment_count)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80866d1a-9583-4427-af01-99b56fa5f27f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# broadcast variable\n",
    "\n",
    "regions = {\"North\": \"N\", \"South\": \"S\", \"East\": \"E\", \"West\": \"W\"}\n",
    "\n",
    "region_broadcast = sc.broadcast(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5c1e05d-1b83-45c9-897e-68a0242002f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# accumulator\n",
    "\n",
    "high_value_count = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2001, 'S', 'Electronics', 838),\n",
       " (2002, 'W', 'Furniture', 471),\n",
       " (2003, 'N', 'Electronics', 803),\n",
       " (2004, 'W', 'Furniture', 174),\n",
       " (2005, 'S', 'Clothing', 590),\n",
       " (2006, 'N', 'Furniture', 937),\n",
       " (2007, 'N', 'Electronics', 391),\n",
       " (2008, 'W', 'Electronics', 961),\n",
       " (2009, 'N', 'Electronics', 305),\n",
       " (2010, 'E', 'Clothing', 213),\n",
       " (2011, 'E', 'Electronics', 615),\n",
       " (2012, 'S', 'Clothing', 573),\n",
       " (2013, 'E', 'Clothing', 352),\n",
       " (2014, 'W', 'Clothing', 768),\n",
       " (2015, 'N', 'Electronics', 231),\n",
       " (2016, 'E', 'Furniture', 217),\n",
       " (2017, 'W', 'Clothing', 346),\n",
       " (2018, 'N', 'Books', 375),\n",
       " (2019, 'S', 'Electronics', 313),\n",
       " (2020, 'W', 'Furniture', 903),\n",
       " (2021, 'W', 'Clothing', 904),\n",
       " (2022, 'N', 'Furniture', 812),\n",
       " (2023, 'W', 'Furniture', 590),\n",
       " (2024, 'N', 'Books', 452),\n",
       " (2025, 'N', 'Books', 697),\n",
       " (2026, 'E', 'Clothing', 959),\n",
       " (2027, 'N', 'Books', 661),\n",
       " (2028, 'S', 'Books', 700),\n",
       " (2029, 'E', 'Clothing', 832),\n",
       " (2030, 'N', 'Furniture', 163),\n",
       " (2031, 'S', 'Books', 413),\n",
       " (2032, 'N', 'Furniture', 952),\n",
       " (2033, 'N', 'Books', 240),\n",
       " (2034, 'E', 'Clothing', 740),\n",
       " (2035, 'S', 'Clothing', 264),\n",
       " (2036, 'W', 'Books', 211),\n",
       " (2037, 'E', 'Clothing', 103),\n",
       " (2038, 'N', 'Electronics', 462),\n",
       " (2039, 'W', 'Electronics', 479),\n",
       " (2040, 'E', 'Electronics', 666),\n",
       " (2041, 'W', 'Clothing', 347),\n",
       " (2042, 'S', 'Electronics', 435),\n",
       " (2043, 'E', 'Clothing', 669),\n",
       " (2044, 'W', 'Electronics', 715),\n",
       " (2045, 'N', 'Electronics', 232),\n",
       " (2046, 'S', 'Electronics', 569),\n",
       " (2047, 'W', 'Books', 989),\n",
       " (2048, 'N', 'Clothing', 507),\n",
       " (2049, 'E', 'Clothing', 429),\n",
       " (2050, 'W', 'Electronics', 679),\n",
       " (2051, 'W', 'Books', 581),\n",
       " (2052, 'W', 'Furniture', 770),\n",
       " (2053, 'S', 'Furniture', 408),\n",
       " (2054, 'E', 'Furniture', 111),\n",
       " (2055, 'S', 'Electronics', 228),\n",
       " (2056, 'N', 'Electronics', 430),\n",
       " (2057, 'S', 'Furniture', 523),\n",
       " (2058, 'S', 'Books', 599),\n",
       " (2059, 'E', 'Clothing', 756),\n",
       " (2060, 'S', 'Electronics', 937),\n",
       " (2061, 'E', 'Electronics', 456),\n",
       " (2062, 'N', 'Books', 238),\n",
       " (2063, 'W', 'Electronics', 210),\n",
       " (2064, 'N', 'Electronics', 922),\n",
       " (2065, 'N', 'Books', 113),\n",
       " (2066, 'N', 'Furniture', 700),\n",
       " (2067, 'S', 'Furniture', 643),\n",
       " (2068, 'S', 'Clothing', 427),\n",
       " (2069, 'W', 'Electronics', 909),\n",
       " (2070, 'W', 'Electronics', 818),\n",
       " (2071, 'N', 'Clothing', 438),\n",
       " (2072, 'E', 'Books', 627),\n",
       " (2073, 'E', 'Furniture', 303),\n",
       " (2074, 'S', 'Clothing', 1000),\n",
       " (2075, 'S', 'Electronics', 453),\n",
       " (2076, 'E', 'Books', 461),\n",
       " (2077, 'S', 'Books', 217),\n",
       " (2078, 'S', 'Clothing', 122),\n",
       " (2079, 'N', 'Clothing', 761),\n",
       " (2080, 'E', 'Books', 783),\n",
       " (2081, 'N', 'Clothing', 135),\n",
       " (2082, 'N', 'Books', 412),\n",
       " (2083, 'W', 'Clothing', 336),\n",
       " (2084, 'W', 'Books', 129),\n",
       " (2085, 'S', 'Clothing', 203),\n",
       " (2086, 'S', 'Furniture', 856),\n",
       " (2087, 'W', 'Books', 507),\n",
       " (2088, 'E', 'Furniture', 681),\n",
       " (2089, 'S', 'Furniture', 887),\n",
       " (2090, 'W', 'Electronics', 295),\n",
       " (2091, 'N', 'Furniture', 777),\n",
       " (2092, 'S', 'Clothing', 913),\n",
       " (2093, 'S', 'Books', 535),\n",
       " (2094, 'N', 'Electronics', 946),\n",
       " (2095, 'W', 'Clothing', 382),\n",
       " (2096, 'N', 'Electronics', 295),\n",
       " (2097, 'W', 'Furniture', 402),\n",
       " (2098, 'S', 'Furniture', 552),\n",
       " (2099, 'S', 'Clothing', 341),\n",
       " (2100, 'W', 'Books', 799)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def enrich_region_and_count(record):\n",
    "    pid, region, category, ammount = record\n",
    "    if ammount > 800:\n",
    "        high_value_count.add(1)\n",
    "    return (pid, region_broadcast.value[region], category, ammount)\n",
    "\n",
    "enriched_records = records.map(enrich_region_and_count)\n",
    "\n",
    "enriched_records.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2001, 'S', 'Electronics', 838),\n",
       " (2002, 'W', 'Furniture', 471),\n",
       " (2003, 'N', 'Electronics', 803),\n",
       " (2004, 'W', 'Furniture', 174),\n",
       " (2005, 'S', 'Clothing', 590),\n",
       " (2006, 'N', 'Furniture', 937),\n",
       " (2007, 'N', 'Electronics', 391),\n",
       " (2008, 'W', 'Electronics', 961),\n",
       " (2009, 'N', 'Electronics', 305),\n",
       " (2010, 'E', 'Clothing', 213),\n",
       " (2011, 'E', 'Electronics', 615),\n",
       " (2012, 'S', 'Clothing', 573),\n",
       " (2013, 'E', 'Clothing', 352),\n",
       " (2014, 'W', 'Clothing', 768),\n",
       " (2015, 'N', 'Electronics', 231),\n",
       " (2016, 'E', 'Furniture', 217),\n",
       " (2017, 'W', 'Clothing', 346),\n",
       " (2018, 'N', 'Books', 375),\n",
       " (2019, 'S', 'Electronics', 313),\n",
       " (2020, 'W', 'Furniture', 903),\n",
       " (2021, 'W', 'Clothing', 904),\n",
       " (2022, 'N', 'Furniture', 812),\n",
       " (2023, 'W', 'Furniture', 590),\n",
       " (2024, 'N', 'Books', 452),\n",
       " (2025, 'N', 'Books', 697),\n",
       " (2026, 'E', 'Clothing', 959),\n",
       " (2027, 'N', 'Books', 661),\n",
       " (2028, 'S', 'Books', 700),\n",
       " (2029, 'E', 'Clothing', 832),\n",
       " (2030, 'N', 'Furniture', 163),\n",
       " (2031, 'S', 'Books', 413),\n",
       " (2032, 'N', 'Furniture', 952),\n",
       " (2033, 'N', 'Books', 240),\n",
       " (2034, 'E', 'Clothing', 740),\n",
       " (2035, 'S', 'Clothing', 264),\n",
       " (2036, 'W', 'Books', 211),\n",
       " (2037, 'E', 'Clothing', 103),\n",
       " (2038, 'N', 'Electronics', 462),\n",
       " (2039, 'W', 'Electronics', 479),\n",
       " (2040, 'E', 'Electronics', 666),\n",
       " (2041, 'W', 'Clothing', 347),\n",
       " (2042, 'S', 'Electronics', 435),\n",
       " (2043, 'E', 'Clothing', 669),\n",
       " (2044, 'W', 'Electronics', 715),\n",
       " (2045, 'N', 'Electronics', 232),\n",
       " (2046, 'S', 'Electronics', 569),\n",
       " (2047, 'W', 'Books', 989),\n",
       " (2048, 'N', 'Clothing', 507),\n",
       " (2049, 'E', 'Clothing', 429),\n",
       " (2050, 'W', 'Electronics', 679),\n",
       " (2051, 'W', 'Books', 581),\n",
       " (2052, 'W', 'Furniture', 770),\n",
       " (2053, 'S', 'Furniture', 408),\n",
       " (2054, 'E', 'Furniture', 111),\n",
       " (2055, 'S', 'Electronics', 228),\n",
       " (2056, 'N', 'Electronics', 430),\n",
       " (2057, 'S', 'Furniture', 523),\n",
       " (2058, 'S', 'Books', 599),\n",
       " (2059, 'E', 'Clothing', 756),\n",
       " (2060, 'S', 'Electronics', 937),\n",
       " (2061, 'E', 'Electronics', 456),\n",
       " (2062, 'N', 'Books', 238),\n",
       " (2063, 'W', 'Electronics', 210),\n",
       " (2064, 'N', 'Electronics', 922),\n",
       " (2065, 'N', 'Books', 113),\n",
       " (2066, 'N', 'Furniture', 700),\n",
       " (2067, 'S', 'Furniture', 643),\n",
       " (2068, 'S', 'Clothing', 427),\n",
       " (2069, 'W', 'Electronics', 909),\n",
       " (2070, 'W', 'Electronics', 818),\n",
       " (2071, 'N', 'Clothing', 438),\n",
       " (2072, 'E', 'Books', 627),\n",
       " (2073, 'E', 'Furniture', 303),\n",
       " (2074, 'S', 'Clothing', 1000),\n",
       " (2075, 'S', 'Electronics', 453),\n",
       " (2076, 'E', 'Books', 461),\n",
       " (2077, 'S', 'Books', 217),\n",
       " (2078, 'S', 'Clothing', 122),\n",
       " (2079, 'N', 'Clothing', 761),\n",
       " (2080, 'E', 'Books', 783),\n",
       " (2081, 'N', 'Clothing', 135),\n",
       " (2082, 'N', 'Books', 412),\n",
       " (2083, 'W', 'Clothing', 336),\n",
       " (2084, 'W', 'Books', 129),\n",
       " (2085, 'S', 'Clothing', 203),\n",
       " (2086, 'S', 'Furniture', 856),\n",
       " (2087, 'W', 'Books', 507),\n",
       " (2088, 'E', 'Furniture', 681),\n",
       " (2089, 'S', 'Furniture', 887),\n",
       " (2090, 'W', 'Electronics', 295),\n",
       " (2091, 'N', 'Furniture', 777),\n",
       " (2092, 'S', 'Clothing', 913),\n",
       " (2093, 'S', 'Books', 535),\n",
       " (2094, 'N', 'Electronics', 946),\n",
       " (2095, 'W', 'Clothing', 382),\n",
       " (2096, 'N', 'Electronics', 295),\n",
       " (2097, 'W', 'Furniture', 402),\n",
       " (2098, 'S', 'Furniture', 552),\n",
       " (2099, 'S', 'Clothing', 341),\n",
       " (2100, 'W', 'Books', 799)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_records_2 = records.map(enrich_region_and_count)\n",
    "\n",
    "enriched_records_2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_value_count.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd to a new file\n",
    "\n",
    "filtered = enriched_records.filter(lambda x:x[3]>800)\n",
    "\n",
    "filtered.map(lambda x: \",\".join(map(str,x))).repartition(2).saveAsTextFile(\"filtered_high_value_sales_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd to a new file\n",
    "\n",
    "filtered = enriched_records.filter(lambda x:x[3]>800)\n",
    "\n",
    "filtered.map(lambda x: \",\".join(map(str,x))).coalesce(1).saveAsTextFile(\"filtered_high_value_sales_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74a76255-acd0-4d9a-8049-206cbd36a2ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Activity:**\n",
    "\n",
    "1. find all product IDs where the amount is greater than 900.\n",
    "2. Find all transactions that belong to the “Furniture” category.\n",
    "3. Count how many transactions belong to the “Electronics” category.\n",
    "4. Find average amount for each category.\n",
    "5. Find the highest amount and the corresponding product ID.\n",
    "6. Find the total number of unique categories.\n",
    "7. For each category, find the product ID with the highest sale.\n",
    "8. Count how many products were sold for less than 300.\n",
    "9. Sort the transactions by amount in descending order.\n",
    "10. Broadcast Variable: Category Discounts\n",
    "\n",
    "```py\n",
    "{\"Electronics\": 0.10, \"Furniture\": 0.15, \"Clothing\": 0.05, \"Books\": 0.20}\n",
    "```\n",
    "11. Accumulator: Count Transactions Below 300\n",
    "12. Filter and Save Results\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03-spark-rdd",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
