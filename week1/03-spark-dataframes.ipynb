{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18c82a99-393b-4e5b-8656-f3a5ec519192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark DataFrames API\n",
    "\n",
    "\n",
    "### Dataframe\n",
    "\n",
    "- DataFrames are ditributed collections of records, all with pre-defined structure(schema - structure and data types of all columns)\n",
    "-  DataFrames are built on Spark's core concepts but with structure, optimization and SQL-like operations for data manipulation.\n",
    "- DataFrames track their schema and provide native support for many common SQL functions and relational operators\n",
    "- DataFrames are evaluated as DAGs, using lazy evaluation and providing lineage and fault tolerance.\n",
    "- DataFrames are immutable\n",
    "\n",
    "### SparkContext vs SparkSession\n",
    "\n",
    "- SparkSession is Spark application entry point. \n",
    "- Introduced in spark 2.0 as a unified entry point for all contexts (formerly instantiated individually as SparkContext, SQLContext, HiveContext, StreamingContext)\n",
    "\n",
    "<i>Note: In databricks it is automatically created for you as spark</i>\n",
    "\n",
    "### DataFrame API Optimizations\n",
    "\n",
    "- **Adaptive Query Execution:** Dynamic plan adjustments during runtime based on actual data characteristics and execution patterns.\n",
    "- **In-Memory Columnar Storage(Tungsten):** In-Memory coloumnar format for all the DataFrames enabling efficient analytical query performance and reduced memory footprint.\n",
    "- **Built-in Statistics** - Automatic statistics collection when saving to optimized formats (Parqurt, Delta in databricks) enables smarter query planning and execution.\n",
    "- **Catalyst Optimizer:** Query optimization engine that coverts DataFrame operations into an optimized execution plan\n",
    "\n",
    "\n",
    "<i>**Note** Databricks comes with a native vectorized query engine that accelerates query execution using photon engine</i>\n",
    "\n",
    "**DataFrame Query Planning:** \n",
    "\n",
    "- When a DataFrame is evaluated, the driver creates an optimized execution plan through a series of transformations \n",
    "- Converts the logical plan into phycal execution that minimizes resource usage and execution time. (Unresolved LP -> analysed LP -> optimized LP -> Physical Plan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "726916e6-0294-4687-b8c5-5823798112de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating DataFrames - DataFrameReader\n",
    "# supports multiple formats such as JSON, CSV, Parquet, ORC, Text or Binary files, existing RDD, and an external db\n",
    "\n",
    "df_cust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7406575f-8750-452e-926d-babb32379f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DataFrame Data Types\n",
    "\n",
    "#### Primitive\n",
    "\n",
    "**`pyspark.sql.types.DataType`**\n",
    "\n",
    "- `ByteType`\n",
    "- `ShortType`\n",
    "- `IntegerType`\n",
    "- `LongType`\n",
    "- `FloatType`\n",
    "- `DoubleType`\n",
    "- `BooleanType`\n",
    "- `StringType`\n",
    "- `BinaryType`\n",
    "- `TimestampType`\n",
    "- `DateType`\n",
    "\n",
    "#### complex data types\n",
    "\n",
    "- `ArrayType`\n",
    "- `MapType`\n",
    "- `StructType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d7b3c54-7108-4391-846e-880a5d93d223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ac0124e-8d24-4df7-8fb6-df46299721f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# custom schema definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0fea9de-a0f3-486a-8bcd-0202908845eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DDL Schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb219e89-64de-4af7-b998-6906d4071e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Common DataFrame API methods\n",
    "\n",
    "#### Transformations\n",
    "\n",
    "##### Narrow Transformations\n",
    "\n",
    "- narrow transformations process data within each partition independetly, without needing to combine data from other partitions.\n",
    "- faster and more efficient because they avoid data shuffling between partitions. \n",
    "\n",
    "1. `select()` : selecting specific rows\n",
    "2. `filter()`: Applying a filter condition to rows. \n",
    "3. `map()`: Applying a function to each row. \n",
    "4. `union()`: Combining two DataFrames with identical schemas. \n",
    "5. `withColumn()`: Adding a new column based on existing ones. \n",
    "6. `drop()`: Removing a column. \n",
    "\n",
    "##### Wide Transformations\n",
    "\n",
    "- Wide transformations require data to be redistributed across partitions, often involving shuffling data based on keys.\n",
    "\n",
    "1. `groupBy()`: Grouping data based on a column, which often requires shuffling to aggregate data from different partitions. \n",
    "2. `join()`: Joining two DataFrames, which requires shuffling data to combine rows based on a join key. \n",
    "3. `distinct()`: Removing duplicate rows, which might require shuffling to compare rows across partitions. \n",
    "\n",
    "#### Actions\n",
    "\n",
    "1. `count()`: returns number of rows in a Dataframe\n",
    "2. `show()`: display DataFrame content\n",
    "3. `take(n)`: return first n rows from a DataFrame\n",
    "4. `first()`: return first row from a DataFrame\n",
    "5. `write()`: save DataFrame to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "350a1de5-384e-417c-a632-a192d9a02b39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Map, Shuffle and Reduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "728ae25e-3955-482f-932d-67dd2497040e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Handiling missing values\n",
    "\n",
    "**common functions**\n",
    "\n",
    "- `isNull()`/`isNotNull()` - checks if values are null\n",
    "- `count(col)` - counts non null values in a specific column\n",
    "- `df.fillna()`/`df.na.fill()` - replace nulls with values\n",
    "- `df.dropna()`/`df.na.drop()` - remove rows with nulls\n",
    "\n",
    "#### referencing columns\n",
    "\n",
    "- direct - `df.select(\"col_name\")` - basic column selection\n",
    "\n",
    "- by attribute - `df.select(df.attribute)` - column names that are valid python idenfiers, can be referenced across DataFrames(e.g., join)\n",
    "\n",
    "- column expression - `df.select(df[\"col_name\"])` - any column names, can be referenced across DataFrames\n",
    "\n",
    "- column object - `df.select(col(\"name\"))` - required when building complex expressions or using column specific operations like `cast()`, `alias()`, `asc()`or `desc()`\n",
    "\n",
    "\n",
    "#### common column object methods\n",
    "\n",
    "- `alias()` - rename column\n",
    "- `cast()` - chnage data type\n",
    "- `isNull()` or `isNotNull()` - check for nulls\n",
    "- `contains()` - string matching\n",
    "- `asc()`/`desc()` - sort direction - `df.sort(col(\"c1\").asc())`\n",
    "\n",
    "\n",
    "[pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.html)\n",
    "\n",
    "#### common built-in functions\n",
    "\n",
    "- `concat(col1, col2)` - concatenate strings\n",
    "- `date_format(col, fmt)` - fromat date strings\n",
    "- `round(col, scale)` - round number to scale\n",
    "- `regexp_replace(col, pattern, replace)` - replace using regex\n",
    "- `coalesce(col1, col2)` - first non null value\n",
    "\n",
    "\n",
    "[pyspark built in functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "\n",
    "#### user defined functions\n",
    "\n",
    "- allows to use python functions on dataframe columns\n",
    "- helpful to create custom reusable functions\n",
    "- can impact performance as they can not be optimized by the Catalyst optimizer and has serialiation overhead.\n",
    "\n",
    "```py\n",
    "@udf(\"data_type\")\n",
    "def function_name(name):\n",
    "    return value\n",
    "\n",
    "df.select(function_name(\"name\"))\n",
    "```\n",
    "\n",
    "<i><b>note:</b> pandas udfs are efficient becuase they operate on batches of raows instead of single rows, they leverage Apache Arrow for more efficient python-JVM serialization </i>\n",
    "\n",
    "### Aggregate functions\n",
    "\n",
    "can be applied in `agg()` method after `groupBy()` operation or directly within `select()`\n",
    "\n",
    "- `sum()`\n",
    "- `avg()`\n",
    "- `min()`\n",
    "- `max()`\n",
    "- `count()`\n",
    "- `first()`\n",
    "- `last()`\n",
    "\n",
    "### Spark SQL\n",
    "\n",
    "- Spark SQL is a module in spark that allows you to run SQL queries on structured and semi-structured data.\n",
    "- It provides a SQL-like interface on top of Spark's powerful DataFrame API, enabling both SQL and programmatic access to data, all with the same optimized execution engine.\n",
    "\n",
    "- `createOrReplaceTempView()` : created a temp in-memory view that you can query with SQL.\n",
    "- `createGlobalTempView()` : Creates a global temporary view accessible across sessions (under global_temp database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6184651c-ef51-44c1-84b9-877a29c9ee11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42d52e90-e59b-4910-b053-d9395ad4ca32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58cd9b28-d5ca-4710-8ed3-f6cdea97176b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94c3085a-c698-4499-b623-618d84923bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc4c5549-51d4-40cb-8aef-b49ed9b5ee5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50b440fd-0031-4c68-8507-bdd753d00581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5625a6a8-c4d3-4e90-b09d-d1d276a767c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e1674bb-8df7-4765-b51c-8e2c66586055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Compare Spark SQL and DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6335edd6-60e5-47ec-a65e-b457285d4a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03-spark-dataframes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
